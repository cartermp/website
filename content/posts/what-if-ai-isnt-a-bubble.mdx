---
title: "What if AI isn't a Bubble?"
description: "I'm not sure people are prepared for a world where AI ends up not being a bubble."
date: "2025-12-15"
tags: ai, llms
---

Earlier this year I claimed that [LLMs are computers](./llms-computers) and that:

1. Just like the early days of PCs, they are not very good yet, but will improve
2. Long term, they may be one of the most impactful aspects of our economies moving forward

I also claimed that LLM is a bit of a misnomer, since modern AI systems are far more than just the "fancy autocomplete" of old, even if that still gets a lot of use.

I'm also probably a believer in the idea that the current form of LLMs based on the Transformer won't be the end-all, be-all of model architectures for AI systems in the future. But much like how the PC industry changed how processors, OS kernels, filesystems, and other things worked, I don't think that's a particularly important point.

## Why is big tech spending the way it is?

A lot of people sure do have their arms in the air regarding the CAPEX of big tech and finance around AI right now. Many claim it's "unsustainable," as if these organizations don't have utterly mind-boggling amounts of wealth to deploy. But I think there's a much simpler answer:

[Big tech believes with strong confidence that AI compute will grow a bunch in the next few years](https://www.dwarkesh.com/p/satya-nadella-2).

Couple that with:

1. Big tech makes much more money than it spends
2. ChatGPT, Gemini, etc. are all adding tons of users every month
3. AI company revenue is going up and to the right, and it's not just model providers

It's really just that simple. The only puzzle to solve for engineers and product people like me is (1) keep reducing compute costs per unit of work, and (2) build valuable products on top of the compute layer. A lot of smart people are doing both things right now, and while (2) will take some time, I have no doubt it will happen as well.

And the tech companies know this. Maybe some investors who believed a revolution would come in 1–2 years are getting scared now that it didn't, but they were always going to be the suckers in this equation anyway. Big tech, for all its faults, knows how to play the long game, and many other investors do too.

## What if it's not a bubble?

I don't think we're prepared for a world where AI isn't a bubble. A lot of people talk about the harms of it bursting: the economy going into a recession, jobs getting lost, and an industry sector getting torched for a decade.

Well, good news on that front—we'll probably get an economic crash from President Trump's tariffs a lot sooner than we will from some AI bubble bursting.

I think a lot of people think about AI as a bubble because the bubble popping and the world moving on from the technology is, ultimately, a predictable and familiar thing to hold in their heads. Most don't recognize that such an outcome would actually result in an abundance of cheap compute for a million new companies to snatch up and build on top of, resulting in probably _more_ AI than we have now—but we can ignore that part.

No, I'm talking about what if most of the things major labs are saying today come true:

What if Claude Code or some other coding system actually _does_ get good enough to hold a whole medium-to-large codebase in context and solve _most_ problems just by being told to? What if AI SREs are good enough to diagnose, root-cause, and hand off fixes to coding agents for most production problems? What if they can also define most of the right signals to know when things go awry?

What if AI systems are so cheap to run at scale that they can automate the lion's share of "enter this data into this spreadsheet and tell me what the results look like"? What if they're accurate enough that you can just do any task 3–4 times in parallel such that, when you compare results, you're pretty much guaranteed a correct answer? What if AI systems get good enough to build most deterministic verification functions for any task they're given, and they just verify answers in a loop until they get the right one?

What if xAI's Grok is able to produce enough realistic pornography with zero guardrails that you can snap a picture of anybody on Earth and get infinite horrific porn of that person? What if OSS models do the same? What if the worst people in the world use this to produce infinite CSAM and plaster it across every part of the web?

What if AI gets good enough at medical diagnoses that enough people consider it immoral to use it only as a supplement? What if self-driving cars finally happen at scale? What if we find that a tailored AI tutor actually improves student outcomes for most kids? What if AI systems craft objectively better policy for local, state, and federal governments? What if our financial sectors run better when mostly automated?

What if drones can automonously operate towards most objectives? What if police and military forces don't require boots on the ground for most things anymore? What if AI + robotics can fill most gaps where they are required?

What if any of the above things get good enough, and a deeply racist, misogynistic, homophic, and so on world leader decides to specialize AI systems towards automated governance along their horrifying political alignment?

**What if only one of these things ends up being true?** Increasingly, I believe that one of them will. And I don't think we're remotely prepared for that reality because we can't even imagine what society would look like if most programmers don't program, drivers don't drive, teacher don't teach, CSAM and other pornographic abuse infects every inch of the web, police forces can _actually_ automate discrimination with specialized AI systems, and so on.

## Ten years from now

I don't think an AI revolution or bubble popping—whether either exists or not—will happen overnight.

1. There's too much money available to pay for AI buildouts to keep it all from halting
2. Even if there's no bubble, it will take time for any number of deeply disruptive things to permeate through society

Sorry to all the "AI 2027" boosters, but I'm not with you on this one. Whether by matter of the technology itself or simply the difficult of _any_ technology to fully move through the world, the AI revolution (if it happens) won't happen in a year. But I find it very hard to think of how AI _doesn't_ drive some critical functions of the world and its people by 2035.